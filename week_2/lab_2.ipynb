{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical Implementation and Hands-on Exercises - Logistic Regression From Scratch\n",
    "\n",
    "In this lab session, we will dive deep into logistic regression, offering a hands-on experience that blends theory with practice. \n",
    "You'll not only build a logistic regression model from scratch but also compare its performance with scikit-learn's LogisticRegression implementation. \n",
    "This approach will reinforce your understanding of the underlying mechanics of logistic regression and provide insight into the effectiveness of your implementation.\n",
    "\n",
    "#### Implementing Logistic Regression from Scratch\n",
    "\n",
    "1. **Overview:** You'll start by implementing logistic regression using a step-by-step approach, focusing on understanding each part of the algorithm, including the logistic function, the cost function, and the gradient descent optimization process.\n",
    "\n",
    "2. **Step-by-Step Guide:**\n",
    "   - **Logistic Function:** Implement the sigmoid function that models the probability as a function of input features.\n",
    "   - **Cost Function:** Code the log loss function to evaluate how well your model fits the data.\n",
    "   - **Gradient Descent:** Write the gradient descent optimization algorithm to minimize the cost function, updating the model's weights.\n",
    "\n",
    "3. **Model Training:** Use a simple, split dataset to train your model. Pay attention to how the choice of learning rate and number of iterations impacts the convergence and performance of your model.\n",
    "\n",
    "\n",
    "#### Applying Logistic Regression with Scikit-learn\n",
    "\n",
    "1. **Quick Application:** Apply logistic regression to the same dataset (Iris) using scikit-learn’s `LogisticRegression` class. This will serve as a benchmark to evaluate the performance of your implementation.\n",
    "2. **Preprocessing Steps:** Review and apply necessary preprocessing steps like feature scaling, which is crucial for logistic regression. Scikit-learn's `StandardScaler` can be used for scaling the features.\n",
    "\n",
    "#### Model Evaluation: Compare and Interpret Results\n",
    "\n",
    "1. **Evaluation Metrics:** Use accuracy, precision, recall, and the F1 score to evaluate the performance of both your model and scikit-learn’s model. This will give you a comprehensive view of how well each model performs.\n",
    "2. **Confusion Matrix:** Generate confusion matrices for both models to visualize true positives, true negatives, false positives, and false negatives.\n",
    "3. **Performance Comparison:**\n",
    "   - **Quantitative Analysis:** Compare the metrics between your logistic regression model and the scikit-learn model. Discuss any differences in performance and speculate on potential reasons for these differences.\n",
    "   - **Qualitative Analysis:** Reflect on the learning experience of implementing logistic regression from scratch. Consider aspects like the complexity of the algorithm, challenges encountered, and the insights gained through manual implementation.\n",
    "\n",
    "Some code is already provided to guide you through the implementation process. You are encouraged to experiment with the code, modify it, and explore additional functionalities to deepen your understanding of logistic regression.\n",
    "\n",
    "There are tags as \"### FILL HERE ###\" in the code blocks, which you need to fill in with the appropriate code. I tried to give as minimal code as possible so that you can understand the concept better. Mainly the code is given to provide structure to the implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid function implementation\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    z = np.clip(x, -250, 250)  # Clipping the input to avoid overflow\n",
    "    ### FILL HERE ###\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost function - Our MLE cost function\n",
    "def cost_function(X, y, weights):\n",
    "    ### FILL HERE ###\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient descent\n",
    "def gradient_descent(X, y, weights, learning_rate, iterations):\n",
    "    ### FILL HERE ###\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction\n",
    "def predict(X, weights, threshold = 0.5):\n",
    "    ### FILL HERE ###\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "def accuracy(y_true, y_pred):\n",
    "    ### FILL HERE ###\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the learning curve\n",
    "def plot_learning_curve(cost_history):\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.plot(range(len(cost_history)), cost_history)\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('Cost')\n",
    "    plt.title('Gradient Descent Learning Curve')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "\n",
    "# Load the iris dataset\n",
    "wine = load_wine()\n",
    "X = wine.data\n",
    "y = wine.target\n",
    "\n",
    "# Feature scaling\n",
    "### FILL HERE ### Hint: Use sklearn\n",
    "\n",
    "# Splitting dataset into training and testing set\n",
    "X_train, X_test, y_train, y_test = None ### FILL HERE ### Hint: sklearn has a function for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the Model\n",
    "X_train_b = np.c_[np.ones((X_train.shape[0], 1)), X_train]  # Add bias column\n",
    "X_test_b = np.c_[np.ones((X_test.shape[0], 1)), X_test]  # Add bias column\n",
    "\n",
    "\n",
    "weights = None  # Initialize the weights    ### FILL HERE ###\n",
    "iterations = None # Number of iterations    ### FILL HERE ###\n",
    "learning_rate = None # Learning rate        ### FILL HERE ###\n",
    "\n",
    "weights, cost_history = gradient_descent(X_train_b, y_train, weights, learning_rate, iterations)\n",
    "\n",
    "print(\"Weights after training:\", weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions\n",
    "y_pred = predict(X_test_b, weights)\n",
    "\n",
    "# Evaluating the model\n",
    "accuracy = None\n",
    "print(\"Model accuracy on the test set:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curve(cost_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing with sklearn's implementation\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "### FILL HERE ###"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
